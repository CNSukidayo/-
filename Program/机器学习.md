# 目录:  
1.监督学习  
2.多维特征  
3.无监督学习  

机器学习分为三类:  
监督学习、无监督学习、强化学习  



## 1.监督学习  
**目录:**  
1.1 监督学习的概念介绍  
1.2 如何训练模型  
1.3 成本函数初识  
1.4 可视化的成本函数  
1.5 梯度下降  
1.6 用于线性回归的梯度下降(综合)  

### 1.1 监督学习的概念介绍
1.监督学习的分类  
监督学习分为两类:回归、分类  
* 回归:就是input一个X就有与之对应的y,并且y的可能是无限多的  
  任何预测诸如200或1.5或-3.2之类的数字的监督学习模型都在解决所谓的回归问题  
  回归有很多模型,线性回归就是回归模型的一种(还有很多其它模型可以解决回归问题)
* 分类:input一个x会有与之对应的y,但是y的个数是有限的,y更类似于一种类别  
  比如图像识别;y可能的取值有cat、dog、mouse  
  解决分类问题的模型就是分类模型,它是监督学习中的另一种模型

2.基本名称介绍  
监督学习中需要使用训练集,在训练集中需要知道如下概念:input、output、m  
监督学习在学习时给定一个input 'x' (输入特征)时同时也会给出其对应的output 'y'(输出目标);训练集的数据量就称之为m(number of training examples)  
可以使用(x,y)来代表单个训练集(single training example)  
比如对于某单个训练集可以表示成:(x,y)=(2104,400)  
假设现在有m个训练集,为了表示其中某个具体的训练集可以在(x,y)的右上角指定一个角标i;例如:(x<sup>i</sup>,y<sup>i</sup>)来代表第i个训练集(0<i<m)  

### 1.2 如何训练模型  
1. 需要将训练集(包括输入特征和输出目标)提供给您的算法  
2. 接着你的监督学习算法会产生一些function,这些function的功能就是对新的输入x和输出进行估计和预测,我们将输出写为y-hat;即y上多一个^;<font color="#00FF00">在机器学习中y-hat是y的估计或预测</font>  
3. 如何表示函数function?  
  假设函数可以写成f<sub>w,b</sub>(x)=wx+b  
  此时只要知道w和b的值,就可以根据输入特征x确定预测y-hat  
  所以这个f<sub>w,b</sub>(x)=wx+b函数意味着这个function是一个以x作为输入的函数,并且根据w和b的值,function将输出预测y-hat的某个值  
  之后就可以将该函数简写为f(x)它就等价于f<sub>w,b</sub>(x)
  *提示:f(x)实际上就是y-hat*  
  ![函数图像](resource/machine%20learning/1.png)
  这里这条图像指代的模型实际上就是线性回归,这是具有一个变量的线性回归(只有一个输入x);这种单变量的线性回归也被称为<font color="#00FF00">单变量线性回归</font>
4. 成本函数  
  * 作用:成本函数用于度量曲线的拟合情况,告诉模型它的运行情况如何
  * 概念介绍:  
    对于线性函数f<sub>w,b</sub>(x)=wx+b  
    其中w和b被称为模型的参数,在机器学习的过程中,<font color="#00FF00">模型参数可以在训练期间进行更改即调参</font>  
    根据不同的参数(w和b),将会得到不同的函数f(x)  
    ![成本函数](resource/machine%20learning/2.png)  
  * 对某个模型进行分析:  
    ![单个模型](resource/machine%20learning/3.png)  
    假设图中的蓝色直线为现在的function(模型),对于训练集x<sup>(i)</sup>它对应的结果为y<sup>(i)</sup>;而模型预测的结果为y-hat<sup>(i)</sup>  
    所以它可以写成下面这种形式:  
    y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>)  
    f<sub>w,b</sub>(x<sup>(i)</sup>)=w</sub>x<sup>(i)</sup>+b  
    可以看到当前模型的预测结果y-hat与真实的结果y是有差距的,<font color="#FF00FF">所以训练模型的目标应当是找到w和b的值,使得对于大部分训练集(x<sup>i</sup>,y<sup>i</sup>)模型的预测结果y-hat<sup>i</sup>能够更加接近y<sup>i</sup></font>  
  * 成本函数(cost function):  
    计算预测结果y-hat与真实结果y之间的距离,即(y-hat<sup>(i)</sup>-y<sup>(i)</sup>)<sup>2</sup>接着需要测量整个训练集的误差,所以要将这些误差值连加即(m是训练集的数量):  
    <font color="#00FF00">$\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>  
    计算平均值:  
    $\frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$
    此时该公式已经可以使用了,但是按照惯例取平均值的时候不是1/m,而是1/2m;所以最终的公式是:<font color="#FF00FF">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>
    J(w,b)指代成本函数  
    不同的应用程序会定义不同的成本函数,但线性回归中上述的这种平方误差是最常用的函数.  
    由于y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>),所以上述公式也可以写成:<font color="#FFC800">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})$</font>  
    <font color="#FF00FF">最终我们要找到使成本函数变小的w和b的值,它可以描述为:</font>**<font color="#FF00FF">minimizeJ(w,b)</font>** 

### 1.3 成本函数分析:  
首先先简化之前的模型;将线性函数线性函数fw,b(x)=wx+b简化为线性函数fw(x)=wx  
那么该模型对应的成本函数为:$J(w) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w}(x{(i)}) - y^{(i)})$  
所以成本函数J(w)实际上是关于w的一个函数  
目标变为:minimizeJ(w)  
<font color="#00FF00">当w的取值为1时</font>,成本函数的结果为:  
![成本函数](resource/learning/4.png)  
图中红叉是训练集,横轴x是input 纵轴y是对应的output
当w=1时f<sub>w</sub>(x)=wx实际上就等价于f(x)=x(该函数曲线正好是图中粉色的线)  
- - -
<font color="#00FF00">当w=0.5时</font>,成本函数的结果为:
![成本函数](resource/learning/5.png)  
此时成本函数的值大约为0.58  
- - -
<font color="#00FF00">当w=0时</font>(w是可以取负数的),成本函数的结果为:  
![成本函数](resource/learning/6.png)  
此时成本函数的值大约为2.3  
图中浅蓝色的直线为<font color="#00FF00">当w=-0.5时</font>的函数图像  
通过计算一系列值,最终可以找到成本函数J的样子(也就是右侧棕色图像)

### 1.4 可视化的成本函数
1.3为了简单简化了成本函数,现在回到一开始的成本函数:f<sub>w,b</sub>(x)=wx+b  
它的图像如下面所示:  
![二维成本函数](resource/learning/7.png)  
此时成本函数J的值就是(w=-10,b=-15)时的高度值  

**可视化之等高线:**  
![可视化等高线](resource/learning/8.png)  
注意看最下面那张图的红色线,假设有一个水果刀按平面切一刀;**此时这一刀切下去的所有点的高度都是相等的**  
再看右上角的椭圆线,每一个椭圆上的点的代价都是相同的,所以这里蓝色、黄色、绿色三个×的成本函数的结果J都是一样的;**对应到左边三个线性模型(蓝色、黄色、绿色直线)它们的成本都是一样的**(即这三个模型的成本是一样的),即使它们的w和b不相同  
<font color="#FF00FF">右上角椭圆的中心值就是成本函数最小的w和b的取值</font>  
**<font color="#FF00FF">通过梯度下降算法可以帮助我们来找到w和b的值</font>**  

### 1.5 梯度下降
1.作用  
**<font color="#FF00FF">通过梯度下降算法可以帮助我们来找到w和b的值</font>**   

2.介绍  
梯度下降是一种可用于最小化任何函数的算法,而不仅仅是线性回归的成本函数,适用于两个以上参数的模型和其他成本函数  
最开始可以给定w和b一些初始值,在线性回归中初始值是多少并不重要,比较常见的选择是将它们都设置为0  
使用梯度下降算法,只需要每次都稍微改变参数w和b以尝试降低w和b参数对应的成本j,直到j稳定或接近最小值  

3.非凸形函数  
**但对于某些非凸形函数,可能存在不止一个最小值;同理凸形函数永远只有一个最小值不存在局部最小值**  
![介绍](resource/learning/9.png)  

4.梯度下降的流程(非凸形函数)  
![梯度下降](resource/learning/10.png)  
* 在梯度下降中你可以选定模型的初始参数,假设这里小人所在的位置对应的w和b是本次梯度下降的初始参数(**<font color="#FF0000">注意这里两个红圈分别对应两个不同的初始输入</font>**)  
* 先看黑色的那条线,在最开始的时候站在高点环顾一周,找到一个能最快下山的下一步目标点,然后移动到目标点再环顾一周,找到下一个能最快下山的目标点,重复这个过程直到你进入山谷
* 可以看到选择不同的起始点(即不同的模型初始参数),最终很有可能得到不同的最低点(最小值);<font color="#FFC800">这里的黑色线和蓝色线就对应两个不同初始值的情况</font>;我们称这种值为**局部最小值**,<font color="#00FF00">并且不能通过局部最小值得到全局最小值</font>  

5.梯度下降的实现  
对于最开始给定的参数w和b,每次做如下操作:  
$$w=w-α\frac{\partial J(w,b)}{\partial w}$$
$$b=b-α\frac{\partial J(w,b)}{\partial b}$$

<font color="#00FF00">其中α被称为学习率,它的取值范围是0-1之间的一个小数;它的作用是控制下降(下坡)的幅度</font>,如果α非常大就会采用非常激进的梯度下降过程,如果α非常小就会采用非常精细的梯度下降过程.  
*注意:这里是对成本函数J(w,b)求偏导而不是对模型求偏导(f<sub>w,b</sub>(x))*

6.理解梯度下降  
再次简化之前的模型,使得现在模型变为f<sub>w</sub>(x)=wx;与之对应的成本函数变为J(w)  
![两个不同的参数w](resource/learning/11.png)  
首先看上面的图,一开始参数w选择在右侧(**红色方框圈出的点**);根据第5节讲的公式对w进行梯度下降,因为α永远是大于0的,而此时对函数j(w)的w求导得到的结果就是该点的切线的斜率,结果也是大于0的.<font color="#00FF00">所以w会变小即往中间收敛</font>  

接着看下面的图,一开始参数w选择在左侧(**红色方框圈出的点**);根据第5节讲的公式对w进行梯度下降,因为α永远是大于0的,而此时对函数j(w)的w求导得到的结果就是该点的切线的斜率,<font color="#FFC800">结果是小于0的</font>.<font color="#00FF00">所以w会变大同样往中间收敛</font>

7.关于学习率α  
**前言:** 学习率α的选择将对实现梯度下降的效率产生巨大的影响  

**α取值大小分析:**
* α取值很小的情况:  
  如果学习率很小,梯度下降能够正常生效,但是速度会非常慢
* α取值很大的情况:  
  由于α很大导致每次跨步很大,所以可能最终也不会达到最小值  
  另一种说法是,α过大可能无法收敛反而会发散(即随着不断的学习可能导致得到越来越差的模型)  

![取值大小](resource/learning/12.png)  

**局部最小值:**  
![局部最小值](resource/learning/13.png)  
对于本图的成本函数J,它有两个局部最小值;当我们的参数w已经来到了图中黄色点处时(w=5);<font color="#00FF00">此时此时无论再执行多少次梯度下降,w始终都是等于5</font>;因为黄色点对应的切线斜率为0(粉红色的线)则导数对应的值也是0,无论你的学习率为多少最终计算的表达式将变为w=w  
**结论:** <font color="#FF00FF">如果模型参数的成本已经达到局部最小值了,那么进一步计算梯度下降是完全没有用的.</font>  


### 1.6 用于线性回归的梯度下降(综合)
**1.概念回顾:**
* 线性回归模型:<font color="#FFC800">f<sub>w,b</sub>(x)=wx+b</font>
* 成本函数:<font color="#FFC800">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})$</font>
* 梯度下降:
  * $w=w-α\frac{\partial J(w,b)}{\partial w}$
    * 其中$\frac{\partial J(w,b)}{\partial w}$求导后的结果为$\frac{1}{m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})(x^{(i)})$
    * $w=w-α\frac{1}{m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})(x^{(i)})$
  * $b=b-α\frac{\partial J(w,b)}{\partial b}$
    * 其中$\frac{\partial J(w,b)}{\partial b}$求导后的结果为$\frac{1}{m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})$
    * 

**2.偏导计算过程:**  
![偏导计算过程](resource/learning/14.png)  

3.运行梯度下降  
* 初始点(蓝色)  
  ![初始点](resource/learning/15.png)  
* 第一步(橙色)
  ![初始点](resource/learning/16.png)  
* 最终结果  
  ![最终结果](resource/learning/17.png)  

## 2.多维特征
**目录:**  
2.1 多维特征  
2.2 多元线性回归的梯度下降  
2.3 特征缩放  
2.4 如何判断梯度下降是否收敛(是否真的起效)  

### 2.1 多维特征
1.多维特征符号介绍  
![多维特征符号介绍](resource/learning/18.png)  
在之前的训练集中,只有一个input即房屋的size它对应的output是房屋的价格  
现在进化到多维,有多个input(如size、房间数量、楼层、年龄)对应一个output房屋的价格  
* x<sub>j</sub>:指代某个特征;如x<sub>2</sub>指代房屋卧室数量这个特诊
* n:特征的个数;本例中n=4
* $\overrightarrow{\mathbf{x}}^{(i)}$:同理还是指代某个训练集;但由于现在是多维特征所以头上用了一个向量的标识  
  注意:训练集是不包含结果的,即$\overrightarrow{\mathbf{x}}^{(2)}$它就指代[1416,3,2,40]这个训练集;ouput是用y<sup>(i)</sup>来表示的
* $\overrightarrow{\mathbf{x}}_j^{(i)}$:指代第i个训练集中的第j个特征

2.多维特征模型  
那么这个多维特征的线性模型是什么样的呢?  
![多维特征](resource/learning/19.png)  
**模型解释:(不重要)**  
假设对于一个房屋,它没有面积没有房间没有楼层没有年龄,它的初始价格就为80  
之后房屋的面积每上升1平方米房屋的价格就增加0.1  
每有一间房间时价格就上涨4  
楼层每提高1层价格就上涨10  
年龄每增加1年价格就下降-2  

3.多维特征模型符号化  
我们可以把模型的参数w定义为一个向量$\overrightarrow{\mathbf{w}}$;同样把特征(input)也定义为一个向量$\overrightarrow{\mathbf{x}}$;数字b不变  
最终可以将模型表达式写成:  
<font color="#FFC800">$f_{(\overrightarrow{\mathbf{w}}, b)}(\overrightarrow{\mathbf{X}}) = \overrightarrow{\mathbf{W}} · \overrightarrow{\mathbf{X}} + b$</font>

![最终的效果](resource/learning/20.png)  
其中向量W·向量X的计算(点积),实际上就是两两相乘;可以看到下面的式子与上面的式子展开后的效果是一样的  
该模型被称为<font color="#00FF00">多元线性回归</font>  

### 2.2 多元线性回归的梯度下降
1.多维模型中梯度下降的介绍  
假设我们已经分别计算出每个参数w<sub>j</sub>它对应的导数d<sub>j</sub>的值,那么每次梯度下降的更新方法如下:  
$w_{j}=w_j-αd_{j}$ (j=1..n)*提示:n是特征数量*  
![多维模型梯度下降实现](resource/learning/21.png)  

2.梯度下降的实现  
![梯度下降的实现](resource/learning/22.png)
图中红色方框就是梯度下降的实现  

3.求导的结果  
即对第2步中$J{(\overrightarrow{\mathbf{w}},b)}$进行求导  

4.多元回归的梯度下降  
![多元回归的梯度下降](resource/learning/23.png)  
**<font color="#FF0000">注意图中红色范围划出的公式</font>**,即对第i个训练集第n个特征训练后的结果公式  

### 2.3 特征缩放
1.作用  
可以使梯度下降运行地更快  
*提示:本章重点讲解的对象是特征(input)*  

2.例子  
首先将模型简化为f<sub>w,b</sub>=w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + b  
其中x1代表房屋的大小size,x2代表房间的数量;这两个特征的取值范围分别为300-2000和0-5  
某个训练集的内容为x1=2000,x2=5,price(output)=500k  
看两种不同情况的取值与结果:
* w1=50,w2=0.1,b=50; price=100050.5k
* w1=0.1,w2=50,b=50; price=500k

显然第二种取值要优于第一种取值  
![特征取值](resource/learning/24.png)

3.结论  
如果一个特征的取值范围很 **<font color="#FF00FF">大</font>** ,那么该特征对应的模型参数w在最开始就尽可能地 **<font color="#FF00FF">小</font>**  
如果一个特征的取值范围很 **<font color="#FF00FF">小</font>** ,那么该特征对应的模型参数w在最开始就尽可能地 **<font color="#FF00FF">大</font>**  

4.第3步的原因  
![原因分析](resource/learning/25.png)  
左侧图是模型函数图像,右侧图是成本函数图像的等高线图  
可以发现对于input特征参数取值范围较大的情况下,对应的模型参数w只要略微的小幅度改动就会对最终的成本造成影响.  
同样对于input特征参数取值范围较小的情况下,对应的模型参数w需要较大幅度的改动才能对最终的成本造成影响.  

5.特征缩放  
![特征缩放](resource/learning/26.png)  
* 上面一组,如果按照原样训练数据,因为等高线又高又瘦梯度下降可能会在它最终找到全局最小值之前来回弹跳很长时间.就像右上方的梯度下降轨迹那样.
* 下面一组,将x1和x2这两个特征进行等比例转换,**使得size特征和bedroom特征之间能对应上**,也就是让他俩1:1;那么最终通过梯度下降找到全局最小值可能就是很丝滑.  

6.特征缩放的方式  
在第5点中x1与x2的特征缩放是如何实现的?  
特征缩放有三种方式:最大值、均值归一化、Z分数归一化
* 最大值:将特征值除以特征值的最大值得到特征缩放后的值  
  例如:size的取值范围为300-2000,将300/2000后得到的结果为0.15;所以size的特征值缩放后的取值范围为**0.15<=x1<=1**  
  同理bedroom的特征值范围为0-5,将0/5后得到的结果为0;所以bedroom的特征值范围为**0<=x2<=1**  
  ![特征缩放](resource/learning/27.png)  
* 均值归一化:在均值归一化中特征值可以为负数,一般情况下取值范围是[-1,1]  
  * 首先求得某项特征x<sub>j</sub>在训练集中的平均值μ1(j=1)  
  * 接着得到该特征对应的特征值计算公式:$x_1 = \frac{x_1 - μ1}{2000-300}$
  * 在本例中得到的特征值取值范围为-0.18<=x1<=0.82;-0.46<=x2<=0.54
  ![均值归一化](resource/learning/28.png)
* Z分数归一化  

7.特征缩放的时机  
**结论:**  
实际上并不是任何时候都需要特征缩放,假如原本某个特征的最大值与最小值的取值范围就比较接近,实际上是不需要进行特征缩放的,具体可以参开下面这张图的例子  
![特征缩放的时机](resource/learning/29.png)  
<font color="#00FF00">图中蓝色字体标注的特征值取值范围是不需要缩放的,红色字体标注的特征值缩放区间是需要缩放的.</font>

### 2.4 如何判断梯度下降是否收敛(是否真的起效)





## 3.无监督学习

无监督学习分为三类:  
聚类、异常检测、降维

聚类:它很强的一点就是它是自主的,大部分情况下它可能会将数据聚类  
* 比如文章的聚类就是通过无监督学习完成的,例如十万篇文章;点击了一篇关于大熊猫的文章,在该文章的下面有推荐文章,这些推荐文章与当前文章之间就是一种组(聚类)关系,那么无监督学习就能分别出这些文章.  
* 再比如对人的基因进行聚类,无监督学习通过学习DNA信息,能够将不喜欢吃西蓝花的人的基因聚类到一起.所以在机器学习给出的某个组中,这组的人可能都有相同的特性.  
  这就是无监督学习,因为我们没有提前告诉算法,某种DNA的人会有哪些特性(而监督学习是会提前告知的)

异常检测:比如可以用于转账的检测看这笔交易是否合法  

降维:将一个大数据集神奇地压缩成一个小得多的数据集,同时丢失尽可能少的信息  

