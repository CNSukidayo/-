# 目录:  
1.监督学习  
2.无监督学习  

机器学习分为三类:  
监督学习、无监督学习、强化学习  

监督学习分为两类:  
回归、分类  

* 回归:就是input一个X就有与之对应的y,并且y的可能是无限多的  
  任何预测诸如200或1.5或-3.2之类的数字的监督学习模型都在解决所谓的回归问题  
  回归有很多模型,线性回归就是回归模型的一种(还有很多其它模型可以解决回归问题)
* 分类:input一个x会有与之对应的y,但是y的个数是有限的,y更类似于一种类别  
  比如图像识别;y可能的取值有cat、dog、mouse  
  解决分类问题的模型就是分类模型,它是监督学习中的另一种模型

## 1.监督学习  
**目录:**  
1.1 监督学习的概念介绍  
1.2 如何训练模型  
1.3 成本函数初识  
1.4 可视化的成本函数  
1.5 梯度下降  

### 1.1 监督学习的概念介绍
监督学习中需要使用训练集,在训练集中需要知道如下概念:input、output、m  
监督学习在学习时给定一个input 'x' (输入特征)时同时也会给出其对应的output 'y'(输出目标);训练集的数据量就称之为m(number of training examples)  
可以使用(x,y)来代表单个训练集(single training example)  
比如对于某单个训练集可以表示成:(x,y)=(2104,400)  
假设现在有m个训练集,为了表示其中某个具体的训练集可以在(x,y)的右上角指定一个角标i;例如:(x<sup>i</sup>,y<sup>i</sup>)来代表第i个训练集(0<i<m)  

### 1.2 如何训练模型  
1. 需要将训练集(包括输入特征和输出目标)提供给您的算法  
2. 接着你的监督学习算法会产生一些function,这些function的功能就是对新的输入x和输出进行估计和预测,我们将输出写为y-hat;即y上多一个^;<font color="#00FF00">在机器学习中y-hat是y的估计或预测</font>  
3. 如何表示函数function?  
  假设函数可以写成f<sub>w,b</sub>(x)=wx+b  
  此时只要知道w和b的值,就可以根据输入特征x确定预测y-hat  
  所以这个f<sub>w,b</sub>(x)=wx+b函数意味着这个function是一个以x作为输入的函数,并且根据w和b的值,function将输出预测y-hat的某个值  
  之后就可以将该函数简写为f(x)它就等价于f<sub>w,b</sub>(x)
  *提示:f(x)实际上就是y-hat*  
  ![函数图像](resource/machine%20learning/1.png)
  这里这条图像指代的模型实际上就是线性回归,这是具有一个变量的线性回归(只有一个输入x);这种单变量的线性回归也被称为<font color="#00FF00">单变量线性回归</font>
4. 成本函数  
  * 作用:成本函数用于度量曲线的拟合情况,告诉模型它的运行情况如何
  * 概念介绍:  
    对于线性函数f<sub>w,b</sub>(x)=wx+b  
    其中w和b被称为模型的参数,在机器学习的过程中,<font color="#00FF00">模型参数可以在训练期间进行更改即调参</font>  
    根据不同的参数(w和b),将会得到不同的函数f(x)  
    ![成本函数](resource/machine%20learning/2.png)  
  * 对某个模型进行分析:  
    ![单个模型](resource/machine%20learning/3.png)  
    假设图中的蓝色直线为现在的function(模型),对于训练集x<sup>(i)</sup>它对应的结果为y<sup>(i)</sup>;而模型预测的结果为y-hat<sup>(i)</sup>  
    所以它可以写成下面这种形式:  
    y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>)  
    f<sub>w,b</sub>(x<sup>(i)</sup>)=w</sub>x<sup>(i)</sup>+b  
    可以看到当前模型的预测结果y-hat与真实的结果y是有差距的,<font color="#FF00FF">所以训练模型的目标应当是找到w和b的值,使得对于大部分训练集(x<sup>i</sup>,y<sup>i</sup>)模型的预测结果y-hat<sup>i</sup>能够更加接近y<sup>i</sup></font>  
  * 成本函数(cost function):  
    计算预测结果y-hat与真实结果y之间的距离,即(y-hat<sup>(i)</sup>-y<sup>(i)</sup>)<sup>2</sup>接着需要测量整个训练集的误差,所以要将这些误差值连加即(m是训练集的数量):  
    <font color="#00FF00">$\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>  
    计算平均值:  
    $\frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$
    此时该公式已经可以使用了,但是按照惯例取平均值的时候不是1/m,而是1/2m;所以最终的公式是:<font color="#FF00FF">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>
    J(w,b)指代成本函数  
    不同的应用程序会定义不同的成本函数,但线性回归中上述的这种平方误差是最常用的函数.  
    由于y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>),所以上述公式也可以写成:<font color="#FFC800">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})$</font>  
    <font color="#FF00FF">最终我们要找到使成本函数变小的w和b的值,它可以描述为:</font>**<font color="#FF00FF">minimizeJ(w,b)</font>** 

### 1.3 成本函数分析:  
首先先简化之前的模型;将线性函数线性函数fw,b(x)=wx+b简化为线性函数fw(x)=wx  
那么该模型对应的成本函数为:$J(w) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w}(x{(i)}) - y^{(i)})$  
所以成本函数J(w)实际上是关于w的一个函数  
目标变为:minimizeJ(w)  
<font color="#00FF00">当w的取值为1时</font>,成本函数的结果为:  
![成本函数](resource/learning/4.png)  
图中红叉是训练集,横轴x是input 纵轴y是对应的output
当w=1时f<sub>w</sub>(x)=wx实际上就等价于f(x)=x(该函数曲线正好是图中粉色的线)  
- - -
<font color="#00FF00">当w=0.5时</font>,成本函数的结果为:
![成本函数](resource/learning/5.png)  
此时成本函数的值大约为0.58  
- - -
<font color="#00FF00">当w=0时</font>(w是可以取负数的),成本函数的结果为:  
![成本函数](resource/learning/6.png)  
此时成本函数的值大约为2.3  
图中浅蓝色的直线为<font color="#00FF00">当w=-0.5时</font>的函数图像  
通过计算一系列值,最终可以找到成本函数J的样子(也就是右侧棕色图像)

### 1.4 可视化的成本函数
1.3为了简单简化了成本函数,现在回到一开始的成本函数:f<sub>w,b</sub>(x)=wx+b  
它的图像如下面所示:  
![二维成本函数](resource/learning/7.png)  
此时成本函数J的值就是(w=-10,b=-15)时的高度值  

**可视化之等高线:**  
![可视化等高线](resource/learning/8.png)  
注意看最下面那张图的红色线,假设有一个水果刀按平面切一刀;**此时这一刀切下去的所有点的高度都是相等的**  
再看右上角的椭圆线,每一个椭圆上的点的代价都是相同的,所以这里蓝色、黄色、绿色三个×的成本函数的结果J都是一样的;**对应到左边三个线性模型(蓝色、黄色、绿色直线)它们的成本都是一样的**(即这三个模型的成本是一样的),即使它们的w和b不相同  
<font color="#FF00FF">右上角椭圆的中心值就是成本函数最小的w和b的取值</font>  
**<font color="#FF00FF">通过梯度下降算法可以帮助我们来找到w和b的值</font>**  

### 1.5 梯度下降
1.作用  
**<font color="#FF00FF">通过梯度下降算法可以帮助我们来找到w和b的值</font>**   

2.介绍  
梯度下降是一种可用于最小化任何函数的算法,而不仅仅是线性回归的成本函数,适用于两个以上参数的模型和其他成本函数  
最开始可以给定w和b一些初始值,在线性回归中初始值是多少并不重要,比较常见的选择是将它们都设置为0  
使用梯度下降算法,只需要每次都稍微改变参数w和b以尝试降低w和b参数对应的成本j,直到j稳定或接近最小值  

3.非凸形函数  
**但对于某些非凸形函数,可能存在不止一个最小值**  
![介绍](resource/learning/9.png)  

4.梯度下降的流程(非凸形函数)  
![梯度下降](resource/learning/10.png)  
* 在梯度下降中你可以选定模型的初始参数,假设这里小人所在的位置对应的w和b是本次梯度下降的初始参数(**<font color="#FF0000">注意这里两个红圈分别对应两个不同的初始输入</font>**)  
* 先看黑色的那条线,在最开始的时候站在高点环顾一周,找到一个能最快下山的下一步目标点,然后移动到目标点再环顾一周,找到下一个能最快下山的目标点,重复这个过程直到你进入山谷
* 可以看到选择不同的起始点(即不同的模型初始参数),最终很有可能得到不同的最低点(最小值);<font color="#FFC800">这里的黑色线和蓝色线就对应两个不同初始值的情况</font>;我们称这种值为**局部最小值**,<font color="#00FF00">并且不能通过局部最小值得到全局最小值</font>  

5.梯度下降的实现  








## 2.无监督学习

无监督学习分为三类:  
聚类、异常检测、降维

聚类:它很强的一点就是它是自主的,大部分情况下它可能会将数据聚类  
* 比如文章的聚类就是通过无监督学习完成的,例如十万篇文章;点击了一篇关于大熊猫的文章,在该文章的下面有推荐文章,这些推荐文章与当前文章之间就是一种组(聚类)关系,那么无监督学习就能分别出这些文章.  
* 再比如对人的基因进行聚类,无监督学习通过学习DNA信息,能够将不喜欢吃西蓝花的人的基因聚类到一起.所以在机器学习给出的某个组中,这组的人可能都有相同的特性.  
  这就是无监督学习,因为我们没有提前告诉算法,某种DNA的人会有哪些特性(而监督学习是会提前告知的)

异常检测:比如可以用于转账的检测看这笔交易是否合法  

降维:将一个大数据集神奇地压缩成一个小得多的数据集,同时丢失尽可能少的信息  

