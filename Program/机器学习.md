机器学习分为三类:  
监督学习、无监督学习、强化学习  

监督学习分为两类:  
回归、分类  

* 回归:就是input一个X就有与之对应的y,并且y的可能是无限多的  
  任何预测诸如200或1.5或-3.2之类的数字的监督学习模型都在解决所谓的回归问题  
  回归有很多模型,线性回归就是回归模型的一种(还有很多其它模型可以解决回归问题)
* 分类:input一个x会有与之对应的y,但是y的个数是有限的,y更类似于一种类别  
  比如图像识别;y可能的取值有cat、dog、mouse  
  解决分类问题的模型就是分类模型,它是监督学习中的另一种模型

### 监督学习  
监督学习中需要使用训练集,在训练集中需要知道如下概念:input、output、m  
监督学习在学习时给定一个input 'x' (输入特征)时同时也会给出其对应的output 'y'(输出目标);训练集的数据量就称之为m(number of training examples)  
可以使用(x,y)来代表单个训练集(single training example)  
比如对于某单个训练集可以表示成:(x,y)=(2104,400)  
假设现在有m个训练集,为了表示其中某个具体的训练集可以在(x,y)的右上角指定一个角标i;例如:(x<sup>i</sup>,y<sup>i</sup>)来代表第i个训练集(0<i<m)  

### 如何训练模型  
1. 需要将训练集(包括输入特征和输出目标)提供给您的算法  
2. 接着你的监督学习算法会产生一些function,这些function的功能就是对新的输入x和输出进行估计和预测,我们将输出写为y-hat;即y上多一个^;<font color="#00FF00">在机器学习中y-hat是y的估计或预测</font>  
3. 如何表示函数function?  
  假设函数可以写成f<sub>w,b</sub>(x)=wx+b  
  此时只要知道w和b的值,就可以根据输入特征x确定预测y-hat  
  所以这个f<sub>w,b</sub>(x)=wx+b函数意味着这个function是一个以x作为输入的函数,并且根据w和b的值,function将输出预测y-hat的某个值  
  之后就可以将该函数简写为f(x)它就等价于f<sub>w,b</sub>(x)
  *提示:f(x)实际上就是y-hat*  
  ![函数图像](resource/machine%20learning/1.png)
  这里这条图像指代的模型实际上就是线性回归,这是具有一个变量的线性回归(只有一个输入x);这种单变量的线性回归也被称为<font color="#00FF00">单变量线性回归</font>
4. 成本函数  
  * 作用:成本函数用于度量曲线的拟合情况,告诉模型它的运行情况如何
  * 概念介绍:  
    对于线性函数f<sub>w,b</sub>(x)=wx+b  
    其中w和b被称为模型的参数,在机器学习的过程中,<font color="#00FF00">模型参数可以在训练期间进行更改即调参</font>  
    根据不同的参数(w和b),将会得到不同的函数f(x)  
    ![成本函数](resource/machine%20learning/2.png)  
  * 对某个模型进行分析:  
    ![单个模型](resource/machine%20learning/3.png)  
    假设图中的蓝色直线为现在的function(模型),对于训练集x<sup>(i)</sup>它对应的结果为y<sup>(i)</sup>;而模型预测的结果为y-hat<sup>(i)</sup>  
    所以它可以写成下面这种形式:  
    y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>)  
    f<sub>w,b</sub>(x<sup>(i)</sup>)=w</sub>x<sup>(i)</sup>+b  
    可以看到当前模型的预测结果y-hat与真实的结果y是有差距的,<font color="#FF00FF">所以训练模型的目标应当是找到w和b的值,使得对于大部分训练集(x<sup>i</sup>,y<sup>i</sup>)模型的预测结果y-hat<sup>i</sup>能够更加接近y<sup>i</sup></font>  
  * 成本函数(cost function):  
    计算预测结果y-hat与真实结果y之间的距离,即(y-hat<sup>(i)</sup>-y<sup>(i)</sup>)<sup>2</sup>接着需要测量整个训练集的误差,所以要将这些误差值连加即(m是训练集的数量):  
    <font color="#00FF00">$\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>  
    计算平均值:  
    $\frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$
    此时该公式已经可以使用了,但是按照惯例取平均值的时候不是1/m,而是1/2m;所以最终的公式是:<font color="#FF00FF">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$</font>
    J(w,b)指代成本函数  
    不同的应用程序会定义不同的成本函数,但线性回归中上述的这种平方误差是最常用的函数.  
    由于y-hat<sup>(i)</sup>=f<sub>w,b</sub>(x<sup>(i)</sup>),所以上述公式也可以写成:<font color="#FFC800">$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})$</font>  
    <font color="#FF00FF">最终我们要找到使成本函数变小的w和b的值,它可以描述为:</font>**<font color="#FF00FF">minimizeJ(w,b)</font>** 
  * 成本函数分析:  
    首先先简化之前的模型;将线性函数线性函数fw,b(x)=wx+b简化为线性函数fw(x)=wx  
    那么该模型对应的成本函数为:$J(w) = \frac{1}{2m} \sum_{i=1}^{m} (f_{w}(x^{(i)}) - y^{(i)})$  
    所以成本函数J(w)实际上是关于w的一个函数  
    目标变为:minimizeJ(w)  
    <font color="#00FF00">当w的取值为1时</font>,成本函数的结果为:  
    ![成本函数](resource/machine%20learning/4.png)  
    图中红叉是训练集,横轴x是input 纵轴y是对应的output
    当w=1时f<sub>w</sub>(x)=wx实际上就等价于f(x)=x(该函数曲线正好是图中粉色的直线)  








无监督学习分为三类:  
聚类、异常检测、降维

聚类:它很强的一点就是它是自主的,大部分情况下它可能会将数据聚类  
* 比如文章的聚类就是通过无监督学习完成的,例如十万篇文章;点击了一篇关于大熊猫的文章,在该文章的下面有推荐文章,这些推荐文章与当前文章之间就是一种组(聚类)关系,那么无监督学习就能分别出这些文章.  
* 再比如对人的基因进行聚类,无监督学习通过学习DNA信息,能够将不喜欢吃西蓝花的人的基因聚类到一起.所以在机器学习给出的某个组中,这组的人可能都有相同的特性.  
  这就是无监督学习,因为我们没有提前告诉算法,某种DNA的人会有哪些特性(而监督学习是会提前告知的)

异常检测:比如可以用于转账的检测看这笔交易是否合法  

降维:将一个大数据集神奇地压缩成一个小得多的数据集,同时丢失尽可能少的信息  

